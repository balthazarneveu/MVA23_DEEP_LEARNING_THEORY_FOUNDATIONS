{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP3 : Uncertainty applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [I.1] Monte Carlo dropout sampling to estimate confidence\n",
    "> Question : Comment results for investigating most uncertain vs confident samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [II] Failure prediction:\n",
    "> Explain the goal of failure prediction\n",
    "\n",
    "First of all, **failure prediction** in an autonomous system is critical as no engineer shall take a system as perfect and shall always design safety and emergency mechanism. \n",
    "\n",
    "If the autonoumous system is over confident or simply does not even tell it's making a mistake, there are actual potential consequences.\n",
    "\n",
    "\n",
    "Here are the 3 main goals I thought of:\n",
    "- Reliability and **build trust** in the system: \n",
    "  - By predicting when a model might fail, we can improve the reliability of machine-learning based systems, especially in critical applications like autonomous cars, healthcare, or finance, where mistakes can have serious consequences.\n",
    "  - A rough \"bad\" example: In the most widespread and popular Machine Learning based technology today being ChatGPT, there are no explicit indications of confidence in the answer. *We see from this lab session that it's not an easy thing either*. Although there are warnings everywhere on the website, `ChatGPT can make mistakes. Consider checking important information.`, you can get wrong answers (wrong content) but in a good form so it looks like a good answer. The issue with such a sometimes *deceptive* system is that you tend to forget it can make mistakes.\n",
    "- **Improve model performances**: Failure prediction can help in identifying weaknesses in a model:\n",
    "   - This insight allows data scientists and engineers to refine and improve the model, either by retraining with more diverse data, tweaking the architecture, or applying different techniques to handle potential failure cases.\n",
    "   - For instance, what we learnt when reviewing the MNIST most confusing example (using MC-dropout based confidence) is: \"how do they look like?\". We basically got a knowledge of what's causing trouble to the network. From there we could try to get more samples of this kind for instance.\n",
    "   - After mining some \"hard examples\", you may start collecting new data to improve your system performances.\n",
    "- **Safety and Risk Management**: \n",
    "  - In safety-critical systems, such as medical diagnosis or industrial automation, predicting failures is crucial for risk management. By understanding when and how a model might fail, steps can be taken to mitigate these risks, either through human intervention or automatic **safeguards**. \n",
    "  - Assessing there's been a failure can even give back the control to a human or another manual system, trigger an emergency etc...\n",
    "   \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Comment the code of the LeNetConfidNet class [II.1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Analyze results between MCP, MCDropout and ConfidNet [II.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [III.1] OOD detection: analyse results and explain the difference between the 3 methods [III.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
